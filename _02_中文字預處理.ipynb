{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "     ---------------------------------------- 0.0/19.2 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/19.2 MB 4.2 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 5.8/19.2 MB 18.5 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 11.3/19.2 MB 22.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 17.0/19.2 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 19.2/19.2 MB 22.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.2-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages (from pandas) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading numpy-2.2.2-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 6.8/12.9 MB 32.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.9 MB 33.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 29.0 MB/s eta 0:00:00\n",
      "Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 7.1/11.1 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 31.6 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.1-cp310-cp310-win_amd64.whl (43.9 MB)\n",
      "   ---------------------------------------- 0.0/43.9 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 7.3/43.9 MB 37.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 15.2/43.9 MB 35.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 23.1/43.9 MB 35.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 31.2/43.9 MB 36.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 38.8/43.9 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.8/43.9 MB 36.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.9/43.9 MB 31.7 MB/s eta 0:00:00\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314474 sha256=a707e96dca48e9aaa717d95f3ac5951471307aca2f80ee25282dcedd6ec7819b\n",
      "  Stored in directory: c:\\users\\student\\appdata\\local\\pip\\cache\\wheels\\c9\\69\\31\\d56d90b22a1777b0b231e234b00302a55be255930f8bd92dcd\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba, tzdata, threadpoolctl, numpy, joblib, scipy, pandas, scikit-learn\n",
      "Successfully installed jieba-0.42.1 joblib-1.4.2 numpy-2.2.2 pandas-2.2.3 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0 tzdata-2025.1\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['這是', '使用', 'Jieba', 'sklearn', '進行', '中', '文字', '預處理', '範例']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 輸入文本\n",
    "text = \"這是使用 Jieba 和 sklearn 進行中文字預處理的範例。\"\n",
    "\n",
    "text = ''.join(c for c in text if c.isalpha())\n",
    "# '這是使用Jieba和sklearn進行中文字預處理的範例'\n",
    "\n",
    "# 分詞\n",
    "tokens = jieba.lcut(text)\n",
    "# ['這是', '使用', 'Jieba', '和', 'sklearn', '進行', '中', '文字', '預處理', '的', '範例']\n",
    "\n",
    "# 去除停用詞\n",
    "stop_words = ['的', '和', '是', '這']\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "# ['這是', '使用', 'Jieba', 'sklearn', '進行', '中', '文字', '預處理', '範例']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['羅傑費德勒',\n",
       " '，',\n",
       " '已',\n",
       " '退役',\n",
       " '瑞士',\n",
       " '男子',\n",
       " '職業',\n",
       " '網球運動員',\n",
       " '，',\n",
       " '費',\n",
       " '德勒',\n",
       " '總共贏',\n",
       " '得',\n",
       " '20',\n",
       " '座',\n",
       " '大滿貫',\n",
       " '冠軍',\n",
       " '，',\n",
       " '單打',\n",
       " '世界排名',\n",
       " '第一',\n",
       " '累計',\n",
       " '310',\n",
       " '周',\n",
       " '，',\n",
       " '其中',\n",
       " '包括',\n",
       " '連續',\n",
       " '237',\n",
       " '周',\n",
       " '世界排名',\n",
       " '第一',\n",
       " '男子',\n",
       " '網壇',\n",
       " '紀錄',\n",
       " '，',\n",
       " '為',\n",
       " '網球史上',\n",
       " '最佳',\n",
       " '男子',\n",
       " '選手',\n",
       " '之一',\n",
       " '。',\n",
       " '費',\n",
       " '德勒',\n",
       " '生涯',\n",
       " '贏得',\n",
       " '103',\n",
       " '個',\n",
       " 'ATP',\n",
       " '單打冠軍',\n",
       " '，',\n",
       " '含',\n",
       " '20',\n",
       " '座',\n",
       " '大滿貫',\n",
       " '冠軍',\n",
       " '6',\n",
       " '座',\n",
       " 'ATP',\n",
       " '年終',\n",
       " '總決賽',\n",
       " '冠軍',\n",
       " '，',\n",
       " '以及',\n",
       " '28',\n",
       " '座',\n",
       " '大師賽',\n",
       " '冠軍',\n",
       " '。']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自訂義字詞\n",
    "# jieba.load_userdict('./dict.txt')\n",
    "\n",
    "# 自己增加字詞\n",
    "jieba.add_word('羅傑費德勒')\n",
    "# jieba.add_word('網球史上')\n",
    "# jieba.add_word('網球運動員')\n",
    "# jieba.add_word('單打冠軍')\n",
    "# jieba.add_word('冠軍')\n",
    "# jieba.add_word('大滿貫')\n",
    "# jieba.add_word('總決賽')\n",
    "# jieba.add_word('大師賽')\n",
    "\n",
    "# 輸入文本\n",
    "text = '''羅傑費德勒，已退役的瑞士男子職業網球運動員，費德勒總共贏得20座大滿貫冠軍，單打世界排名第一累計310周，其中包括連續237周世界排名第一的男子網壇紀錄，為網球史上最佳的男子選手之一。費德勒生涯贏得103個ATP單打冠軍，含20座大滿貫冠軍和6座ATP年終總決賽冠軍，以及28座大師賽冠軍。'''\n",
    "\n",
    "# 分詞\n",
    "tokens = jieba.lcut(text)\n",
    "\n",
    "# 停用詞\n",
    "stop_words = ['的', '和', '是', '這']\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
